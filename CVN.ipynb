{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub==0.3.3\n",
      "  Using cached kagglehub-0.3.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\s\\anaconda3\\envs\\cse151a\\lib\\site-packages (from kagglehub==0.3.3) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\s\\anaconda3\\envs\\cse151a\\lib\\site-packages (from kagglehub==0.3.3) (2.32.3)\n",
      "Collecting tqdm (from kagglehub==0.3.3)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\s\\anaconda3\\envs\\cse151a\\lib\\site-packages (from requests->kagglehub==0.3.3) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\s\\anaconda3\\envs\\cse151a\\lib\\site-packages (from requests->kagglehub==0.3.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\s\\anaconda3\\envs\\cse151a\\lib\\site-packages (from requests->kagglehub==0.3.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\s\\anaconda3\\envs\\cse151a\\lib\\site-packages (from requests->kagglehub==0.3.3) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\s\\anaconda3\\envs\\cse151a\\lib\\site-packages (from tqdm->kagglehub==0.3.3) (0.4.6)\n",
      "Using cached kagglehub-0.3.3-py3-none-any.whl (42 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, kagglehub\n",
      "Successfully installed kagglehub-0.3.3 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub==0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n"
     ]
    }
   ],
   "source": [
    "# Download latest version of kaggle dataset\n",
    "path = kagglehub.dataset_download(\"alistairking/recyclable-and-household-waste-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image paths and create lists to populate later\n",
    "class_path = path + '/images' + '/images'\n",
    "classes = os.listdir(class_path)\n",
    "image_paths = []\n",
    "labels = [] # Trash category associated with each image\n",
    "d_r = [] # default (studio/standard) vs real world img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access image folders and populate lists with needed data\n",
    "'''Arthur's shit\n",
    "for i, label in enumerate(classes):\n",
    "    class_dir = os.path.join(class_path, label)\n",
    "    for subfolder in ['default', 'real_world']:\n",
    "        subfolder_dir = os.path.join(class_dir, subfolder)\n",
    "        image_names = os.listdir(subfolder_dir)\n",
    "        \n",
    "        for j in range(0, len(image_names), int(len(image_names)/3)): # TODO: change step on turn-in\n",
    "          image_name = image_names[j]\n",
    "          d_r.append(subfolder)\n",
    "          image_paths.append(os.path.join(subfolder_dir, image_name)) \n",
    "          labels.append(label)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access image folders and populate lists with needed data\n",
    "# Serena's shit\n",
    "for i, label in enumerate(classes):\n",
    "    class_dir = os.path.join(class_path, label)\n",
    "    for subfolder in ['default', 'real_world']:\n",
    "        subfolder_dir = os.path.join(class_dir, subfolder)\n",
    "        image_names = os.listdir(subfolder_dir)\n",
    "        \n",
    "        for image_name in image_names:\n",
    "            d_r.append(subfolder)\n",
    "            image_paths.append(os.path.join(subfolder_dir, image_name))\n",
    "            labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all data into df\n",
    "columnDict = {'image_path': image_paths, 'label': labels, 'default_vs_real_world': d_r}\n",
    "df = pd.DataFrame(columnDict)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>default_vs_real_world</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>aerosol_cans</td>\n",
       "      <td>default</td>\n",
       "      <td>recyclable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>aerosol_cans</td>\n",
       "      <td>default</td>\n",
       "      <td>recyclable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>aerosol_cans</td>\n",
       "      <td>default</td>\n",
       "      <td>recyclable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>aerosol_cans</td>\n",
       "      <td>default</td>\n",
       "      <td>recyclable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>aerosol_cans</td>\n",
       "      <td>default</td>\n",
       "      <td>recyclable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path         label  \\\n",
       "0  C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...  aerosol_cans   \n",
       "1  C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...  aerosol_cans   \n",
       "2  C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...  aerosol_cans   \n",
       "3  C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...  aerosol_cans   \n",
       "4  C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...  aerosol_cans   \n",
       "\n",
       "  default_vs_real_world    category  \n",
       "0               default  recyclable  \n",
       "1               default  recyclable  \n",
       "2               default  recyclable  \n",
       "3               default  recyclable  \n",
       "4               default  recyclable  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = df.copy()\n",
    "clean_df.label.unique()\n",
    "cat_dic = {\n",
    "    'landfill' : ['clothing', 'disposable_plastic_cutlery', 'plastic_shopping_bags', 'plastic_trash_bags',\\\n",
    "                  'shoes', 'styrofoam_cups', 'styrofoam_food_containers', 'paper_cups', 'plastic_straws', 'plastic_cup_lids'],\n",
    "    'recyclable' : ['aerosol_cans','aluminum_food_cans', 'aluminum_soda_cans', 'cardboard_boxes', 'cardboard_packaging', 'glass_beverage_bottles',\\\n",
    "                    'glass_cosmetic_containers', 'glass_food_jars', 'magazines', 'newspaper', 'office_paper', 'plastic_detergent_bottles',\\\n",
    "                    'plastic_food_containers','plastic_soda_bottles', 'plastic_water_bottles', 'steel_food_cans'],\n",
    "    'compost' : ['coffee_grounds', 'eggshells', 'food_waste', 'tea_bags']\n",
    "}\n",
    "\n",
    "# Create a reverse lookup dictionary to map items to categories\n",
    "item_to_category = {item: category for category, items in cat_dic.items() for item in items}\n",
    "\n",
    "# Map the labels to respective categories using lookup dictionary\n",
    "clean_df['category'] = df['label'].map(item_to_category)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3750, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subset(df, num):\n",
    "    \"\"\"\n",
    "    Gets subset of dataframe. USE THIS FOR WORKING LOCALLY ONLY.\n",
    "\n",
    "    ARGS:\n",
    "        df: dataframe \n",
    "        num(float): decimal representing what amt of original df you wanna use for subset (eg. .5 or .3)\n",
    "    RETURNS:\n",
    "        df: smaller df that's shuffled\n",
    "    \"\"\"\n",
    "    return df.sample(frac=num, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "# CHANGE frac TO 1 WHEN USING SUPER COMPUTER \n",
    "frac = 0.25 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "tiny_df = subset(clean_df, frac)\n",
    "tiny_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2250, Validation size: 1875, Test size: 1875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>recyclable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3411</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>landfill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>recyclable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>recyclable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...</td>\n",
       "      <td>landfill</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path    category\n",
       "1190  C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...  recyclable\n",
       "3411  C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...    landfill\n",
       "530   C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...  recyclable\n",
       "2501  C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...  recyclable\n",
       "49    C:\\Users\\S\\.cache\\kagglehub\\datasets\\alistairk...    landfill"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_val_test(df):\n",
    "    \"\"\"\n",
    "    Split df into train, val, test. HARDCODED to 60% train, 20% val and 20% test btw\n",
    "    \"\"\"\n",
    "    # Get rid of unnecessary columns HARDCODED\n",
    "    df = df[['image_path', 'category']]\n",
    "    # Split into train and temp (40% for validation + test)\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "    \n",
    "    # Split temp into validation and test (10% each)\n",
    "    val_df, test_df = train_test_split(df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = train_val_test(tiny_df)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2250 validated image filenames belonging to 3 classes.\n",
      "Found 1875 validated image filenames belonging to 3 classes.\n",
      "Found 1875 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "def to_tensorflow(df):\n",
    "    \"\"\"\n",
    "    Convert pandas df to tensorflow dataset.\n",
    "    ARGS:\n",
    "        df: pandas df\n",
    "    RETURNS \n",
    "        gen: tensorflow dataset\n",
    "    \"\"\"\n",
    "    preprocess = tf.keras.applications.vgg16.preprocess_input # preprocessing function for CNN \n",
    "    target_size=(224,224) # set the size of the images\n",
    "    color_mode='rgb' # set the type of image\n",
    "    class_mode= 'categorical' # set the class mode\n",
    "    batch_size=10  # set the batch size\n",
    "    shuffle=False # don't shuffle cuz we alrdy shuffled our df\n",
    "    gen=ImageDataGenerator(preprocessing_function=preprocess).flow_from_dataframe(df, \n",
    "          x_col='image_path',\n",
    "          y_col='category', target_size=target_size, color_mode=color_mode,\n",
    "          class_mode=class_mode, batch_size=batch_size, shuffle=shuffle)\n",
    "    return gen\n",
    "pandas_dfs = [train_df, val_df, test_df]\n",
    "train_batches, val_batches, test_batches = [to_tensorflow(df) for df in pandas_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=valid_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['cat', 'dog'], batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
